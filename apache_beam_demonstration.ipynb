{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade apache-beam scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:        3.8.16\n",
      "Apache Beam:   2.60.0\n",
      "Interactive:   True\n",
      "BASE_DIR:      /content/beam_demo\n",
      "DATA_DIR:      /content/beam_demo/data\n",
      "OUT_DIR:       /content/beam_demo/out\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Project directories (local to Colab)\n",
    "BASE_DIR = \"/content/beam_demo\"\n",
    "DATA_DIR = f\"{BASE_DIR}/data\"\n",
    "OUT_DIR  = f\"{BASE_DIR}/out\"\n",
    "for d in (BASE_DIR, DATA_DIR, OUT_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Try to enable Interactive Beam (nice for inspecting PCollections in notebooks)\n",
    "try:\n",
    "    from apache_beam.runners.interactive import interactive_beam as ib\n",
    "    ib.watch(locals())\n",
    "    INTERACTIVE_READY = True\n",
    "except Exception:\n",
    "    INTERACTIVE_READY = False  # It's ok if this isn't available\n",
    "\n",
    "# Default to DirectRunner (local)\n",
    "beam_options = PipelineOptions([\"--runner=DirectRunner\"])\n",
    "\n",
    "# Helper: clean output directory between runs (we'll use this later)\n",
    "def reset_out_dir():\n",
    "    import shutil\n",
    "    if os.path.exists(OUT_DIR):\n",
    "        shutil.rmtree(OUT_DIR)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Summary printout\n",
    "print(f\"Python:        {platform.python_version()}\")\n",
    "print(f\"Apache Beam:   {beam.__version__}\")\n",
    "print(f\"Interactive:   {INTERACTIVE_READY}\")\n",
    "print(f\"BASE_DIR:      {BASE_DIR}\")\n",
    "print(f\"DATA_DIR:      {DATA_DIR}\")\n",
    "print(f\"OUT_DIR:       {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote 200 rows to: /content/beam_demo/data\\transactions.csv\n",
      "event_time,user_id,amount,category\n",
      "2025-10-26T23:00:10.721210Z,7,24.49,grocery\n",
      "2025-10-26T23:00:11.721210Z,20,75.93,utilities\n",
      "2025-10-26T23:00:20.721210Z,29,44.63,utilities\n",
      "2025-10-26T23:00:28.721210Z,15,23.55,travel\n",
      "2025-10-26T23:00:36.721210Z,11,15.4,utilities\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os, random\n",
    "\n",
    "# Make results reproducible\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 200  # number of rows\n",
    "categories = [\"grocery\", \"electronics\", \"fashion\", \"travel\", \"utilities\", \"other\"]\n",
    "\n",
    "start_time = datetime.utcnow() - timedelta(minutes=10)\n",
    "times = []\n",
    "t = start_time\n",
    "for i in range(N):\n",
    "    # irregular gaps between 1-20 seconds to make windows interesting\n",
    "    t += timedelta(seconds=random.randint(1, 20))\n",
    "    times.append(t)\n",
    "\n",
    "user_ids = np.random.randint(1, 31, size=N)  # 30 users\n",
    "# positive skew for amounts; clamp to [1, 500]\n",
    "amounts = np.clip(np.random.lognormal(mean=2.5, sigma=0.8, size=N), 1, 500)\n",
    "cats = np.random.choice(categories, size=N, p=[0.25,0.15,0.2,0.15,0.15,0.1])\n",
    "\n",
    "csv_path = os.path.join(DATA_DIR, \"transactions.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"event_time\",\"user_id\",\"amount\",\"category\"])\n",
    "    for ts, uid, amt, cat in zip(times, user_ids, amounts, cats):\n",
    "        w.writerow([ts.isoformat() + \"Z\", int(uid), round(float(amt), 2), cat])\n",
    "\n",
    "# Quick preview: show first 5 lines\n",
    "print(f\" Wrote {N} rows to: {csv_path}\")\n",
    "with open(csv_path, \"r\") as f:\n",
    "    head = [next(f).strip() for _ in range(6)]  # header + 5 rows\n",
    "print(\"\\n\".join(head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote parsed data to: /content/beam_demo/out\\01_parsed\\transactions-00000-of-00001.jsonl\n",
      "{\"event_time\": \"2025-10-26T23:00:10.721210Z\", \"user_id\": 7, \"amount\": 24.49, \"category\": \"grocery\"}\n",
      "{\"event_time\": \"2025-10-26T23:00:11.721210Z\", \"user_id\": 20, \"amount\": 75.93, \"category\": \"utilities\"}\n",
      "{\"event_time\": \"2025-10-26T23:00:20.721210Z\", \"user_id\": 29, \"amount\": 44.63, \"category\": \"utilities\"}\n",
      "{\"event_time\": \"2025-10-26T23:00:28.721210Z\", \"user_id\": 15, \"amount\": 23.55, \"category\": \"travel\"}\n",
      "{\"event_time\": \"2025-10-26T23:00:36.721210Z\", \"user_id\": 11, \"amount\": 15.4, \"category\": \"utilities\"}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Read CSV -> Map/Filter -> Write JSONL ============================\n",
    "# Demonstrates: Pipeline I/O (ReadFromText, WriteToText), Map, Filter\n",
    "import os, json, csv, glob\n",
    "import apache_beam as beam\n",
    "reset_out_dir()  # from Cell 1\n",
    "INPUT_CSV = os.path.join(DATA_DIR, \"transactions.csv\")\n",
    "OUT_SUBDIR = os.path.join(OUT_DIR, \"01_parsed\")\n",
    "os.makedirs(OUT_SUBDIR, exist_ok=True)\n",
    "OUT_PREFIX = os.path.join(OUT_SUBDIR, \"transactions\")\n",
    "def parse_line(line: str):\n",
    "    # Robust CSV parsing for a single line\n",
    "    row = next(csv.reader([line]))\n",
    "    event_time, user_id, amount, category = row\n",
    "    return {\n",
    "        \"event_time\": event_time,\n",
    "        \"user_id\": int(user_id),\n",
    "        \"amount\": float(amount),\n",
    "        \"category\": category\n",
    "    }\n",
    "def is_valid(rec):  # Removed `: dict` type hint\n",
    "    return rec[\"user_id\"] > 0 and rec[\"amount\"] > 0.0 and rec[\"category\"] != \"\"\n",
    "with beam.Pipeline(options=beam_options) as p:\n",
    "    _ = (\n",
    "        p\n",
    "        # skip_header_lines avoids manual header filtering\n",
    "        | \"ReadCSV\" >> beam.io.ReadFromText(INPUT_CSV, skip_header_lines=1)\n",
    "        | \"ParseCSV\" >> beam.Map(parse_line)          # Map\n",
    "        | \"FilterInvalid\" >> beam.Filter(is_valid)    # Filter\n",
    "        | \"ToJSON\" >> beam.Map(json.dumps)\n",
    "        | \"WriteJSONL\" >> beam.io.WriteToText(\n",
    "            OUT_PREFIX, file_name_suffix=\".jsonl\", num_shards=1\n",
    "        )\n",
    "    )\n",
    "# Preview a few output lines\n",
    "out_files = sorted(glob.glob(os.path.join(OUT_SUBDIR, \"transactions-*.jsonl\")))\n",
    "print(f\" Wrote parsed data to: {out_files[0] if out_files else '(none)'}\")\n",
    "if out_files:\n",
    "    with open(out_files[0], \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5: break\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Enriched output: /content/beam_demo/out\\02_enriched\\transactions_enriched-00000-of-00001.jsonl\n",
      "{\"event_time\": \"2025-10-26T23:00:10.721210Z\", \"user_id\": 7, \"amount\": 24.49, \"category\": \"grocery\", \"amount_tier\": \"low\", \"user_bucket\": 7, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:11.721210Z\", \"user_id\": 20, \"amount\": 75.93, \"category\": \"utilities\", \"amount_tier\": \"med\", \"user_bucket\": 0, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:20.721210Z\", \"user_id\": 29, \"amount\": 44.63, \"category\": \"utilities\", \"amount_tier\": \"low\", \"user_bucket\": 9, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:28.721210Z\", \"user_id\": 15, \"amount\": 23.55, \"category\": \"travel\", \"amount_tier\": \"low\", \"user_bucket\": 5, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:36.721210Z\", \"user_id\": 11, \"amount\": 15.4, \"category\": \"utilities\", \"amount_tier\": \"low\", \"user_bucket\": 1, \"is_high_value\": false}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: ParDo (DoFn) + Composite transform ===============================\n",
    "# Demonstrates: ParDo via a custom DoFn, and a Composite PTransform\n",
    "# Input: transactions.csv\n",
    "# Output: enriched JSONL in OUT_DIR/02_enriched\n",
    "\n",
    "import os, json, csv, glob\n",
    "from typing import Dict, Iterable\n",
    "import apache_beam as beam\n",
    "\n",
    "INPUT_CSV = os.path.join(DATA_DIR, \"transactions.csv\")\n",
    "OUT_SUBDIR = os.path.join(OUT_DIR, \"02_enriched\")\n",
    "os.makedirs(OUT_SUBDIR, exist_ok=True)\n",
    "OUT_PREFIX = os.path.join(OUT_SUBDIR, \"transactions_enriched\")\n",
    "\n",
    "# --- Re-define parse/validate for self-contained execution in this cell -------\n",
    "def parse_line(line: str) -> Dict:\n",
    "    row = next(csv.reader([line]))\n",
    "    event_time, user_id, amount, category = row\n",
    "    return {\n",
    "        \"event_time\": event_time,\n",
    "        \"user_id\": int(user_id),\n",
    "        \"amount\": float(amount),\n",
    "        \"category\": category,\n",
    "    }\n",
    "\n",
    "def is_valid(rec: Dict) -> bool:\n",
    "    return rec[\"user_id\"] > 0 and rec[\"amount\"] > 0.0 and rec[\"category\"] != \"\"\n",
    "\n",
    "# --- ParDo: enrich each record with derived features --------------------------\n",
    "class EnrichDoFn(beam.DoFn):\n",
    "    def process(self, rec: Dict) -> Iterable[Dict]:\n",
    "        rec = dict(rec)  # shallow copy to avoid mutating input\n",
    "        amt = rec.get(\"amount\", 0.0)\n",
    "\n",
    "        # Simple tiering logic based on amount\n",
    "        if amt >= 200:\n",
    "            tier = \"vip\"\n",
    "        elif amt >= 100:\n",
    "            tier = \"high\"\n",
    "        elif amt >= 50:\n",
    "            tier = \"med\"\n",
    "        else:\n",
    "            tier = \"low\"\n",
    "        rec[\"amount_tier\"] = tier\n",
    "\n",
    "        # Normalize and add a user hash bucket (0..9) for later partitioning\n",
    "        rec[\"category\"] = str(rec.get(\"category\", \"\")).strip().lower()\n",
    "        rec[\"user_bucket\"] = int(rec[\"user_id\"]) % 10\n",
    "\n",
    "        # Example boolean flag\n",
    "        rec[\"is_high_value\"] = amt >= 100.0\n",
    "\n",
    "        yield rec\n",
    "\n",
    "# --- Composite transform: parse -> validate -> enrich -------------------------\n",
    "class ParseValidateEnrich(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"ParseCSV\" >> beam.Map(parse_line)\n",
    "            | \"FilterInvalid\" >> beam.Filter(is_valid)\n",
    "            | \"Enrich\" >> beam.ParDo(EnrichDoFn())\n",
    "        )\n",
    "\n",
    "# --- Run pipeline -------------------------------------------------------------\n",
    "with beam.Pipeline(options=beam_options) as p:\n",
    "    lines = p | \"ReadCSV\" >> beam.io.ReadFromText(INPUT_CSV, skip_header_lines=1)\n",
    "    enriched = lines | \"PVE\" >> ParseValidateEnrich()\n",
    "    _ = (\n",
    "        enriched\n",
    "        | \"ToJSON\" >> beam.Map(json.dumps)\n",
    "        | \"WriteJSONL\" >> beam.io.WriteToText(\n",
    "            OUT_PREFIX, file_name_suffix=\".jsonl\", num_shards=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Preview output\n",
    "out_files = sorted(glob.glob(os.path.join(OUT_SUBDIR, \"transactions_enriched-*.jsonl\")))\n",
    "print(f\" Enriched output: {out_files[0] if out_files else '(none)'}\")\n",
    "if out_files:\n",
    "    with open(out_files[0], \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5: break\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Partition complete\n",
      "High-value file: /content/beam_demo/out\\03_partition\\high_value-00000-of-00001.jsonl (records: 3)\n",
      "Regular  file:   /content/beam_demo/out\\03_partition\\regular-00000-of-00001.jsonl  (records: 197)\n",
      "\n",
      "-- High-value sample --\n",
      "{\"event_time\": \"2025-10-26T23:03:21.721210Z\", \"user_id\": 21, \"amount\": 128.37, \"category\": \"other\", \"amount_tier\": \"high\", \"user_bucket\": 1, \"is_high_value\": true}\n",
      "{\"event_time\": \"2025-10-26T23:15:57.721210Z\", \"user_id\": 26, \"amount\": 141.02, \"category\": \"travel\", \"amount_tier\": \"high\", \"user_bucket\": 6, \"is_high_value\": true}\n",
      "{\"event_time\": \"2025-10-26T23:18:34.721210Z\", \"user_id\": 25, \"amount\": 112.97, \"category\": \"fashion\", \"amount_tier\": \"high\", \"user_bucket\": 5, \"is_high_value\": true}\n",
      "\n",
      "-- Regular sample --\n",
      "{\"event_time\": \"2025-10-26T23:00:10.721210Z\", \"user_id\": 7, \"amount\": 24.49, \"category\": \"grocery\", \"amount_tier\": \"low\", \"user_bucket\": 7, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:11.721210Z\", \"user_id\": 20, \"amount\": 75.93, \"category\": \"utilities\", \"amount_tier\": \"med\", \"user_bucket\": 0, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:20.721210Z\", \"user_id\": 29, \"amount\": 44.63, \"category\": \"utilities\", \"amount_tier\": \"low\", \"user_bucket\": 9, \"is_high_value\": false}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Partition (high_value vs regular) ================================\n",
    "# Demonstrates: beam.Partition\n",
    "# Input: OUT_DIR/02_enriched/transactions_enriched-*.jsonl (from Cell 4)\n",
    "# Output: OUT_DIR/03_partition/{high_value|regular}-*.jsonl\n",
    "\n",
    "import os, json, glob\n",
    "import apache_beam as beam\n",
    "\n",
    "# Locate enriched input from previous cell\n",
    "ENRICHED_DIR = os.path.join(OUT_DIR, \"02_enriched\")\n",
    "enriched_files = sorted(glob.glob(os.path.join(ENRICHED_DIR, \"transactions_enriched-*.jsonl\")))\n",
    "assert enriched_files, \"No enriched files found. Please run Cell 4 first.\"\n",
    "INPUT_ENRICHED = enriched_files[0]\n",
    "\n",
    "# Output paths\n",
    "OUT_SUBDIR = os.path.join(OUT_DIR, \"03_partition\")\n",
    "os.makedirs(OUT_SUBDIR, exist_ok=True)\n",
    "OUT_PREFIX_HIGH = os.path.join(OUT_SUBDIR, \"high_value\")\n",
    "OUT_PREFIX_REG  = os.path.join(OUT_SUBDIR, \"regular\")\n",
    "\n",
    "def to_obj(line: str):\n",
    "    return json.loads(line)\n",
    "\n",
    "def partition_by_value(rec, n_partitions):\n",
    "    # 0 -> high_value, 1 -> regular\n",
    "    return 0 if rec.get(\"is_high_value\", False) else 1\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as p:\n",
    "    objs = (\n",
    "        p\n",
    "        | \"ReadEnriched\" >> beam.io.ReadFromText(INPUT_ENRICHED)\n",
    "        | \"ParseJSON\"    >> beam.Map(to_obj)\n",
    "    )\n",
    "\n",
    "    parts = objs | \"PartitionHighRegular\" >> beam.Partition(partition_by_value, 2)\n",
    "\n",
    "    high = parts[0]\n",
    "    reg  = parts[1]\n",
    "\n",
    "    _ = (\n",
    "        high\n",
    "        | \"HighToJSON\" >> beam.Map(json.dumps)\n",
    "        | \"WriteHigh\"  >> beam.io.WriteToText(OUT_PREFIX_HIGH, file_name_suffix=\".jsonl\", num_shards=1)\n",
    "    )\n",
    "    _ = (\n",
    "        reg\n",
    "        | \"RegToJSON\" >> beam.Map(json.dumps)\n",
    "        | \"WriteReg\"  >> beam.io.WriteToText(OUT_PREFIX_REG, file_name_suffix=\".jsonl\", num_shards=1)\n",
    "    )\n",
    "\n",
    "# Preview & simple counts\n",
    "high_file = sorted(glob.glob(os.path.join(OUT_SUBDIR, \"high_value-*.jsonl\")))[0]\n",
    "reg_file  = sorted(glob.glob(os.path.join(OUT_SUBDIR, \"regular-*.jsonl\")))[0]\n",
    "\n",
    "def count_lines(path):\n",
    "    c = 0\n",
    "    with open(path, \"r\") as f:\n",
    "        for _ in f:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "print(\" Partition complete\")\n",
    "print(f\"High-value file: {high_file} (records: {count_lines(high_file)})\")\n",
    "print(f\"Regular  file:   {reg_file}  (records: {count_lines(reg_file)})\")\n",
    "\n",
    "# Show a couple of examples from each\n",
    "print(\"\\n-- High-value sample --\")\n",
    "with open(high_file, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3: break\n",
    "        print(line.strip())\n",
    "\n",
    "print(\"\\n-- Regular sample --\")\n",
    "with open(reg_file, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3: break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Windowing complete. Output: /content/beam_demo/out\\04_windowed\\windowed-00000-of-00001.jsonl\n",
      "{\"window_start\": \"2025-10-26T23:00:00\", \"window_end\": \"2025-10-26T23:01:00\", \"category\": \"grocery\", \"total_amount\": 31.97}\n",
      "{\"window_start\": \"2025-10-26T23:01:00\", \"window_end\": \"2025-10-26T23:02:00\", \"category\": \"grocery\", \"total_amount\": 30.03}\n",
      "{\"window_start\": \"2025-10-26T23:03:00\", \"window_end\": \"2025-10-26T23:04:00\", \"category\": \"grocery\", \"total_amount\": 5.07}\n",
      "{\"window_start\": \"2025-10-26T23:04:00\", \"window_end\": \"2025-10-26T23:05:00\", \"category\": \"grocery\", \"total_amount\": 6.19}\n",
      "{\"window_start\": \"2025-10-26T23:05:00\", \"window_end\": \"2025-10-26T23:06:00\", \"category\": \"grocery\", \"total_amount\": 27.36}\n",
      "{\"window_start\": \"2025-10-26T23:07:00\", \"window_end\": \"2025-10-26T23:08:00\", \"category\": \"grocery\", \"total_amount\": 11.37}\n",
      "{\"window_start\": \"2025-10-26T23:09:00\", \"window_end\": \"2025-10-26T23:10:00\", \"category\": \"grocery\", \"total_amount\": 41.5}\n",
      "{\"window_start\": \"2025-10-26T23:10:00\", \"window_end\": \"2025-10-26T23:11:00\", \"category\": \"grocery\", \"total_amount\": 33.09}\n",
      "{\"window_start\": \"2025-10-26T23:12:00\", \"window_end\": \"2025-10-26T23:13:00\", \"category\": \"grocery\", \"total_amount\": 51.08}\n",
      "{\"window_start\": \"2025-10-26T23:13:00\", \"window_end\": \"2025-10-26T23:14:00\", \"category\": \"grocery\", \"total_amount\": 50.95}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Windowing with event time ================================\n",
    "# Uses TimestampedValue instead of beam.WithTimestamps\n",
    "\n",
    "import os, json, glob\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms import window as beam_window\n",
    "\n",
    "# Locate enriched input\n",
    "ENRICHED_DIR = os.path.join(OUT_DIR, \"02_enriched\")\n",
    "enriched_files = sorted(glob.glob(os.path.join(ENRICHED_DIR, \"transactions_enriched-*.jsonl\")))\n",
    "assert enriched_files, \"No enriched files found. Please run Cell 4 first.\"\n",
    "INPUT_ENRICHED = enriched_files[0]\n",
    "\n",
    "# Output path\n",
    "OUT_SUBDIR = os.path.join(OUT_DIR, \"04_windowed\")\n",
    "os.makedirs(OUT_SUBDIR, exist_ok=True)\n",
    "OUT_PREFIX = os.path.join(OUT_SUBDIR, \"windowed\")\n",
    "\n",
    "def to_obj(line: str):\n",
    "    return json.loads(line)\n",
    "\n",
    "def parse_event_ts(rec):\n",
    "    # Convert ISO8601 '...Z' to timezone-aware and return epoch seconds (float)\n",
    "    s = rec[\"event_time\"]\n",
    "    if s.endswith(\"Z\"):\n",
    "        s = s[:-1] + \"+00:00\"\n",
    "    dt = datetime.fromisoformat(s)\n",
    "    return dt.timestamp()\n",
    "\n",
    "def add_timestamp(rec):\n",
    "    # Attach event-time timestamp to each record\n",
    "    return beam.window.TimestampedValue(rec, parse_event_ts(rec))\n",
    "\n",
    "class FormatWindowedDoFn(beam.DoFn):\n",
    "    def process(self, kv, window=beam.DoFn.WindowParam):\n",
    "        category, total = kv\n",
    "        ws = window.start.to_utc_datetime().isoformat()\n",
    "        we = window.end.to_utc_datetime().isoformat()\n",
    "        yield json.dumps({\n",
    "            \"window_start\": ws,\n",
    "            \"window_end\": we,\n",
    "            \"category\": category,\n",
    "            \"total_amount\": round(float(total), 2),\n",
    "        })\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as p:\n",
    "    _ = (\n",
    "        p\n",
    "        | \"ReadEnriched\" >> beam.io.ReadFromText(INPUT_ENRICHED)\n",
    "        | \"ParseJSON\"    >> beam.Map(to_obj)\n",
    "        | \"AddTimestamps\" >> beam.Map(add_timestamp)                           # event time\n",
    "        | \"FixedWindows60s\" >> beam.WindowInto(beam_window.FixedWindows(60))   # windowing\n",
    "        | \"ToKV\" >> beam.Map(lambda rec: (rec[\"category\"], rec[\"amount\"]))\n",
    "        | \"SumPerCategoryPerWindow\" >> beam.CombinePerKey(sum)\n",
    "        | \"FormatWindowed\" >> beam.ParDo(FormatWindowedDoFn())\n",
    "        | \"WriteWindowed\" >> beam.io.WriteToText(OUT_PREFIX, file_name_suffix=\".jsonl\", num_shards=1)\n",
    "    )\n",
    "\n",
    "# Preview a few windowed results\n",
    "out_files = sorted(glob.glob(os.path.join(OUT_SUBDIR, \"windowed-*.jsonl\")))\n",
    "print(f\" Windowing complete. Output: {out_files[0] if out_files else '(none)'}\")\n",
    "if out_files:\n",
    "    with open(out_files[0], \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 10: break\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artifact check\n",
      " - parsed    : /content/beam_demo/out\\01_parsed\\transactions-00000-of-00001.jsonl  (records: 200)\n",
      " - enriched  : /content/beam_demo/out\\02_enriched\\transactions_enriched-00000-of-00001.jsonl  (records: 200)\n",
      " - high_value: /content/beam_demo/out\\03_partition\\high_value-00000-of-00001.jsonl  (records: 3)\n",
      " - regular   : /content/beam_demo/out\\03_partition\\regular-00000-of-00001.jsonl  (records: 197)\n",
      " - windowed  : /content/beam_demo/out\\04_windowed\\windowed-00000-of-00001.jsonl  (records: 125)\n",
      "\n",
      "🔎 Samples (first 3 lines each):\n",
      "\n",
      "--- PARSED ---\n",
      "{\"event_time\": \"2025-10-26T23:00:10.721210Z\", \"user_id\": 7, \"amount\": 24.49, \"category\": \"grocery\"}\n",
      "{\"event_time\": \"2025-10-26T23:00:11.721210Z\", \"user_id\": 20, \"amount\": 75.93, \"category\": \"utilities\"}\n",
      "{\"event_time\": \"2025-10-26T23:00:20.721210Z\", \"user_id\": 29, \"amount\": 44.63, \"category\": \"utilities\"}\n",
      "\n",
      "--- ENRICHED ---\n",
      "{\"event_time\": \"2025-10-26T23:00:10.721210Z\", \"user_id\": 7, \"amount\": 24.49, \"category\": \"grocery\", \"amount_tier\": \"low\", \"user_bucket\": 7, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:11.721210Z\", \"user_id\": 20, \"amount\": 75.93, \"category\": \"utilities\", \"amount_tier\": \"med\", \"user_bucket\": 0, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:20.721210Z\", \"user_id\": 29, \"amount\": 44.63, \"category\": \"utilities\", \"amount_tier\": \"low\", \"user_bucket\": 9, \"is_high_value\": false}\n",
      "\n",
      "--- HIGH_VALUE ---\n",
      "{\"event_time\": \"2025-10-26T23:03:21.721210Z\", \"user_id\": 21, \"amount\": 128.37, \"category\": \"other\", \"amount_tier\": \"high\", \"user_bucket\": 1, \"is_high_value\": true}\n",
      "{\"event_time\": \"2025-10-26T23:15:57.721210Z\", \"user_id\": 26, \"amount\": 141.02, \"category\": \"travel\", \"amount_tier\": \"high\", \"user_bucket\": 6, \"is_high_value\": true}\n",
      "{\"event_time\": \"2025-10-26T23:18:34.721210Z\", \"user_id\": 25, \"amount\": 112.97, \"category\": \"fashion\", \"amount_tier\": \"high\", \"user_bucket\": 5, \"is_high_value\": true}\n",
      "\n",
      "--- REGULAR ---\n",
      "{\"event_time\": \"2025-10-26T23:00:10.721210Z\", \"user_id\": 7, \"amount\": 24.49, \"category\": \"grocery\", \"amount_tier\": \"low\", \"user_bucket\": 7, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:11.721210Z\", \"user_id\": 20, \"amount\": 75.93, \"category\": \"utilities\", \"amount_tier\": \"med\", \"user_bucket\": 0, \"is_high_value\": false}\n",
      "{\"event_time\": \"2025-10-26T23:00:20.721210Z\", \"user_id\": 29, \"amount\": 44.63, \"category\": \"utilities\", \"amount_tier\": \"low\", \"user_bucket\": 9, \"is_high_value\": false}\n",
      "\n",
      "--- WINDOWED ---\n",
      "{\"window_start\": \"2025-10-26T23:00:00\", \"window_end\": \"2025-10-26T23:01:00\", \"category\": \"grocery\", \"total_amount\": 31.97}\n",
      "{\"window_start\": \"2025-10-26T23:01:00\", \"window_end\": \"2025-10-26T23:02:00\", \"category\": \"grocery\", \"total_amount\": 30.03}\n",
      "{\"window_start\": \"2025-10-26T23:03:00\", \"window_end\": \"2025-10-26T23:04:00\", \"category\": \"grocery\", \"total_amount\": 5.07}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Notebook index + validation =====================================\n",
    "# Summarizes what we've built, verifies outputs exist, and shows quick samples.\n",
    "\n",
    "import os, glob, json, textwrap\n",
    "\n",
    "def count_lines(path):\n",
    "    c = 0\n",
    "    with open(path, \"r\") as f:\n",
    "        for _ in f: c += 1\n",
    "    return c\n",
    "\n",
    "def show_head(path, n=3):\n",
    "    rows = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n: break\n",
    "            rows.append(line.strip())\n",
    "    return rows\n",
    "\n",
    "# Locate artifacts from previous cells\n",
    "artifacts = {\n",
    "    \"parsed\": sorted(glob.glob(os.path.join(OUT_DIR, \"01_parsed\", \"transactions-*.jsonl\"))),\n",
    "    \"enriched\": sorted(glob.glob(os.path.join(OUT_DIR, \"02_enriched\", \"transactions_enriched-*.jsonl\"))),\n",
    "    \"high_value\": sorted(glob.glob(os.path.join(OUT_DIR, \"03_partition\", \"high_value-*.jsonl\"))),\n",
    "    \"regular\": sorted(glob.glob(os.path.join(OUT_DIR, \"03_partition\", \"regular-*.jsonl\"))),\n",
    "    \"windowed\": sorted(glob.glob(os.path.join(OUT_DIR, \"04_windowed\", \"windowed-*.jsonl\"))),\n",
    "}\n",
    "\n",
    "print(\" Artifact check\")\n",
    "for key, files in artifacts.items():\n",
    "    if files:\n",
    "        p = files[0]\n",
    "        print(f\" - {key:10s}: {p}  (records: {count_lines(p)})\")\n",
    "    else:\n",
    "        print(f\" - {key:10s}: MISSING (run the earlier cell for this stage)\")\n",
    "\n",
    "# Show a few sample lines from each available artifact\n",
    "print(\"\\n🔎 Samples (first 3 lines each):\")\n",
    "for key in [\"parsed\", \"enriched\", \"high_value\", \"regular\", \"windowed\"]:\n",
    "    files = artifacts[key]\n",
    "    if not files:\n",
    "        continue\n",
    "    p = files[0]\n",
    "    print(f\"\\n--- {key.upper()} ---\")\n",
    "    for line in show_head(p, n=3):\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
